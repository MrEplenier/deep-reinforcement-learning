{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Trex Run - DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrEplenier/deep-reinforcement-learning/blob/master/Trex_Run_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RPzkHYpvWL1",
        "colab_type": "text"
      },
      "source": [
        "## **Mode d'emploi**\n",
        "\n",
        "Veuillez vous référer au mode d'emploi en préambule du script intitulé *\\\"Trex Run - Double DQN\"*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOpFrALjWduP",
        "colab_type": "text"
      },
      "source": [
        "## Installation et importation des packages "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exM8_kBMWduS",
        "colab_type": "text"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svrU0VV-WduT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install opencv-python #Open-CV (traitement image)\n",
        "!pip install selenium #Selenium (communication avec le navigateur)\n",
        "!pip install Pillow #Pillow (capture de l'image)\n",
        "!pip install tensorflow # Tensorflow (Machine-learning)\n",
        "!pip install Keras # Keras (API Tensorflow)\n",
        "!pip install matplotlib # Matplotlib (sortie des graphiques)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv68SxQ8WduX",
        "colab_type": "text"
      },
      "source": [
        "### Importation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gGMjv6vWduY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import selenium\n",
        "import PIL\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageGrab \n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import numpy as np \n",
        "from collections import deque\n",
        "import random\n",
        "import pickle # sauvegarde de données\n",
        "import keras\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Conv2D, Activation, Dense, Flatten\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWDancz4Wdud",
        "colab_type": "text"
      },
      "source": [
        "## Paramètres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVB-voOaWdue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discount_rate = 0.95 \n",
        "epsilon_min = 0.0001 \n",
        "Epsilon = 0.1 \n",
        "REPLAY_MEMORY = 30000 \n",
        "BATCH = 32 \n",
        "epsilon_decay=0.9995\n",
        "LEARNING_RATE = 1e-3\n",
        "ACTIONS = 2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL1tcIpqWduh",
        "colab_type": "text"
      },
      "source": [
        "## Environnement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyMO1mW8Wdui",
        "colab_type": "text"
      },
      "source": [
        "- Game class: Interface entre le navigateur et python à l'aide de Selenium"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0AwnWBxWduk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Game:\n",
        "    def __init__(self,custom_config=True):\n",
        "        self._driver = webdriver.Chrome(executable_path =\"C:/Users/Thibault/Desktop/Mathieu - RL/chromedriver\")\n",
        "        self._driver.set_window_position(x=-10,y=0) #on positionne la fenetre \n",
        "        self._driver.set_window_size(200, 300) #on redimensionne la taille de la fenêtre\n",
        "        self._driver.get('chrome://dino') #liens de la page du jeu \n",
        "        self._driver.execute_script(\"Runner.config.ACCELERATION=0\") #on annule l'accélération initiale du jeu\n",
        "    \n",
        "    def get_crashed(self): #récupère le game over\n",
        "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
        "    \n",
        "    def restart(self): #redémarre la partie\n",
        "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
        "        \n",
        "        time.sleep(0.25)# pas d'actions et d'apprentissage possible pendant 0.25 secondes après le début du jeu\n",
        "                    \n",
        "    def press_up(self): # faire sauter l'agent\n",
        "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
        "    \n",
        "    def get_score(self):#récuperer le score \n",
        "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
        "        score = ''.join(score_array) \n",
        "        return int(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtOoCVrHWdun",
        "colab_type": "text"
      },
      "source": [
        "### Dinosaure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_TX5q75Wduo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DinoAgent:\n",
        "    def __init__(self,game):\n",
        "        self._game = game; \n",
        "        self.jump(); #Saute pour lancer le jeu\n",
        "        time.sleep(.5) # aucune actions possible pour la premiere fois que le jeu se lance pendant 0.5 secondes\n",
        "    \n",
        "    def is_crashed(self): #récupère quand l'agent s'est heurté a un obstacle\n",
        "        return self._game.get_crashed()\n",
        "    \n",
        "    def jump(self): #fait sauter le dinausore \n",
        "        self._game.press_up()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gINp2tLdWdus",
        "colab_type": "text"
      },
      "source": [
        "- Game state est la fonction principale qui sert de liaison entre l'agent, l'environnement et le code\n",
        "- get_state(): prends en entrée un array d'action et sert a effectuer les actions de l'agent, retourne le nouvel état, récompenses et si le jeu est terminé."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm1EnP1PWdut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Game_sate:\n",
        "    def __init__(self,agent,game):  #créer les objets qui vont nous servir dans la suite du code (environnement et agent)\n",
        "        self._agent = agent \n",
        "        self._game = game\n",
        "        \n",
        "    def get_state(self,actions):\n",
        "        score = self._game.get_score()\n",
        "        is_over = False \n",
        "        if actions[0]==1: # ne rien faire\n",
        "            reward = 1 \n",
        "        if actions[1] == 1: # sauter\n",
        "            self._agent.jump()\n",
        "            reward = -5\n",
        "        image = grab_screen() # récupération des images après chaque action\n",
        "        \n",
        "        if self._agent.is_crashed():\n",
        "            reward = -100 \n",
        "            self._game.restart()\n",
        "            is_over = True #Game over\n",
        "            \n",
        "        return image, score, reward, is_over "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILTTbtFDWduw",
        "colab_type": "text"
      },
      "source": [
        "## Traitement des images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY05Uhs-Wdux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grab_screen(_driver = None):\n",
        "    screen =  np.array(ImageGrab.grab(bbox=(30,120,450,255))) # Capture l'image dans la zone d'écran décrite dans la bbox\n",
        "    image = process_img(screen) \n",
        "    return image\n",
        "    \n",
        "def process_img(image):\n",
        "    \n",
        "    # on redimensionne les images\n",
        "    image = cv2.resize(image, (0,0), fx = 0.50, fy = 0.60) \n",
        "    image = image[::2,::2] #on divise le nombre de pixels de l'image par deux \n",
        "    image = cv2.Canny(image, threshold1 = 100, threshold2 = 200) #permet de déterminer les contours dans l'image en supprimant les détails inutiles et de taille insignifiant (nuage par exemple)\n",
        "    return  image \n",
        "\n",
        "img_rows , img_cols = 105,41 # dimension des images \n",
        "img_channels = 4 # nombre d'images par analyse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt4JN0sKWdu0",
        "colab_type": "text"
      },
      "source": [
        "## Structure du Modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Otak5oSWdu1",
        "colab_type": "text"
      },
      "source": [
        "Le modèle sera détaillé dans le rapport."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yw1BY-2Wdu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buildmodel():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (8, 8), strides=4, padding='same',input_shape=(img_cols,img_rows,img_channels)))  #19*40*4\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64, (4, 4), strides=2, padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64, (3, 3), strides=1, padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(ACTIONS,activation='linear'))\n",
        "    model.compile(loss='mse',optimizer=Adam(lr=LEARNING_RATE))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhnzBPuYWdu4",
        "colab_type": "text"
      },
      "source": [
        "## Entrainement du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz3GZ1NKWdu5",
        "colab_type": "text"
      },
      "source": [
        "Cette partie sera également détaillé dans le rapport."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB3TVU9mWdu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainBatch(minibatch,s_t,model):\n",
        "    \n",
        "    # lot de quatre images en input\n",
        "    inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))\n",
        "    targets = np.zeros((inputs.shape[0], ACTIONS))\n",
        "    for i in range(0, len(minibatch)):                \n",
        "        state_t = minibatch[i][0]    \n",
        "        action_t = minibatch[i][1]   \n",
        "        reward_t = minibatch[i][2]   \n",
        "        state_t1 = minibatch[i][3]   \n",
        "        terminal = minibatch[i][4]   \n",
        "        inputs[i:i + 1] = state_t    \n",
        "        targets[i] = model.predict(state_t)  \n",
        "        Q_sa = model.predict(state_t1)\n",
        "        if terminal:\n",
        "            targets[i, action_t] = reward_t \n",
        "        else:\n",
        "            targets[i, action_t] = reward_t + discount_rate * np.max(Q_sa)\n",
        "    model.train_on_batch(inputs, targets) #equivalent a un model.fit sur les vecteurs  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuRPLXO9WdvA",
        "colab_type": "text"
      },
      "source": [
        "## Entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck3MPBLeWdvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainNetwork(model,game_state,D,M):\n",
        "    # la première action est de ne rien faire lorsque l'on lance l'entrainement\n",
        "    do_nothing = np.zeros(ACTIONS)\n",
        "    do_nothing[0] =1 #0 => ne rien faire\n",
        "    \n",
        "    epsilon=Epsilon\n",
        "                     \n",
        "    x_t, scor0 , r_0, terminal = game_state.get_state(do_nothing) # on récupère le résultat de cette première action pour initialiser l'entrainement\n",
        "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2).reshape(1,41,105,4) # stocke 4 images  redimensionnée en 1*41*105*4 pour initialisé la position\n",
        "\n",
        "    #on boucle sur 30 millions de partie afin qu'il ne s'arrête pas n'importe quand\n",
        "    for j in range(30000000):\n",
        "        action_index = 0\n",
        "        scor=0 #score \n",
        "        r_t = 0 #reward a l'instant t\n",
        "        while (r_t>=-5): #finit la partie quand la reward est inférieur a -5\n",
        "    \n",
        "            a_t = np.zeros([ACTIONS]) # vecteur action à l'instant t     \n",
        "            \n",
        "            if  random.random() <= epsilon: #on choisit une action aléatoire si on tire un nombre aléatoire inférieur a epsilon  \n",
        "                action_index = random.randrange(ACTIONS) #on tire un entier aléatoire dans (0,1)\n",
        "                a_t[action_index] = 1 # si action_index=0: ne rien faire, si =1 : sauter\n",
        "                \n",
        "            \n",
        "            else: # on prédit à l'aide du modèle quelle serait la \"meilleure\" action a faire notre instant t\n",
        "                q = model.predict(s_t)       #on prédit les récompenses associés à chaque action à l'instant t (q_value) \n",
        "                action_index = np.argmax(q)         # on prends l'index de la plus grande récompense\n",
        "                a_t[action_index] = 1        # si action_index=0: ne rien faire, si =1 : sauter\n",
        "                \n",
        "            #on execute l'action déterminée précédemment et on regarde ce qu'elle a \n",
        "            #produit comme reward ainsi que le nouvel état de notre agent\n",
        "            x_t1, scor, r_t, terminal = game_state.get_state(a_t)\n",
        "            \n",
        "            x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #créer une image de dimension 1*41*105*1\n",
        "            s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # ajoiute la nouvelle image concernant la position de l'agent en enlevant la plus ancienne des 4 précèdents.\n",
        "\n",
        "            \n",
        "            D.append((s_t, action_index, r_t, s_t1, terminal)) # stockage des choix des actions au temps t et ce qu'elles ont entrainée\n",
        "            \n",
        "            if len(D) > REPLAY_MEMORY: D.popleft() #on enleve les actions les plus anciennes\n",
        "               \n",
        "            s_t = s_t1 #on passe a l'instant t=t+1\n",
        "        \n",
        "        #on réduit l'epsilon a chaque fin de partie une fois que le modèle a enregistré 750 tentatives\n",
        "        if epsilon > epsilon_min and len(M)>750:\n",
        "            epsilon *= epsilon_decay  \n",
        "        \n",
        "        #de même on lance l'apprentissage une fois que le modèle a 750 parties\n",
        "        if len(M)>750:trainBatch(random.sample(D, BATCH),s_t,model)\n",
        "        \n",
        "       \n",
        "        M.append(scor)\n",
        "        print(\"Partie numéro\",len(M), \"/ EPSILON\", \"%.2f\" % (epsilon*100),\"%\", \"/ score\", scor)\n",
        "        \n",
        "        #sauvegarde de sécurité des actions tous les 100 due a des erreurs réccurentes de pickle \n",
        "        \n",
        "        if (j+1)% 100 == 0:\n",
        "            pickle.dump(D, open('C:/Users/Thibault/Desktop/SaveDino/ActionsDino2', 'wb'))\n",
        "        #Toutes les 30 parties, on sauvegarde notre modèle et nos listes de résultats et d'actions\n",
        "        \n",
        "        if (j+1)% 30 == 0:\n",
        "            #sauvegarde des actions\n",
        "            pickle.dump(D, open('C:/Users/Thibault/Desktop/SaveDino/ActionsDino', 'wb'))\n",
        "            #sauvegarde des scores\n",
        "            pickle.dump(M, open('C:/Users/Thibault/Desktop/SaveDino/ScoreDino', 'wb'))\n",
        "            #sauvegarde du modele\n",
        "            model.save('C:/Users/Thibault/Desktop/SaveDino/save.h5')\n",
        "            \n",
        "            #on trace un graphique pour voir l'évolution de l'apprentissage\n",
        "            x=range(len(M))\n",
        "            y=M\n",
        "            reg=np.polyfit(x,y,10) #regression polynomiale d'ordre 10 sur l'histogramme d'apprentissage\n",
        "            reg_coeff=np.poly1d(reg)\n",
        "            plt.plot(x,y)\n",
        "            plt.plot(x,reg_coeff(x))\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "\n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "X5XD5OsNWdvD",
        "colab_type": "text"
      },
      "source": [
        "## Lancer un nouvel entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJWEpdLMWdvE",
        "colab_type": "code",
        "colab": {},
        "outputId": "e24f6401-8ca5-4178-e118-f0ba1fd56863"
      },
      "source": [
        "Epsilon = 0.1 \n",
        "######## Fonction pour lancer le jeu à 0 #######\n",
        "def NewGame(observe=False):\n",
        "    \n",
        "        # on charge les différentes données de l'environnement (Jeu, Dino, état et rewards)\n",
        "    game = Game()\n",
        "    dino = DinoAgent(game)\n",
        "    game_state = Game_sate(dino,game)\n",
        "    \n",
        "    # on charge la fonction pour avoir un model vierge\n",
        "    model = buildmodel()\n",
        "    \n",
        "    #on initialise les variables contenant les scores et les actions\n",
        "    D=deque()\n",
        "    M=[]\n",
        "    \n",
        "    #on lance le jeu \n",
        "    trainNetwork(model,game_state,D,M)\n",
        "\n",
        "######### end #########\n",
        "\n",
        "#on lance le programme \n",
        "NewGame(False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Partie numéro 1 / EPSILON 10.00 % / score 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz1fpPJiWdvI",
        "colab_type": "text"
      },
      "source": [
        "## Reprendre un entraînement "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_3y3-T6LWdvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Epsilon=0.005 #on reprends le epsilon d'ou l'on souhaite repartir\n",
        "\n",
        "######## Fonction pour relancer le jeu après récupération d'un entrainement #######\n",
        "def continuer(observe=False):\n",
        "    \n",
        "    # on charge les différentes données de l'environnement (Jeu, Dino, état et rewards)\n",
        "    game = Game()\n",
        "    dino = DinoAgent(game)\n",
        "    game_state = Game_sate(dino,game)\n",
        "    \n",
        "    #on télécharge les données sauvegardées lors de l'entrainement précédent:\n",
        "    \n",
        "    ##### Les Actions du modèle #####\n",
        "    D=pickle.load(open('C:/Users/Thibault/Desktop/SaveDino/ActionsDino.', 'rb'))\n",
        "        \n",
        "    ##### Les précédents scores pour avoir une continuité dans la courbe #####\n",
        "    M=pickle.load(open('C:/Users/Thibault/Desktop/SaveDino/ScoreDino', 'rb'))\n",
        "    \n",
        "    #### Le modèle qui a déjà commencé son entrainement (on charge les poids déjà présents) #####\n",
        "    model = load_model('C:/Users/Thibault/Desktop/SaveDino/save.h5')\n",
        "    \n",
        "    #on lance le jeu\n",
        "    trainNetwork(model,game_state,D,M)\n",
        "\n",
        "############ end ############    \n",
        "    \n",
        "#on lance le programme\n",
        "continuer(False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}